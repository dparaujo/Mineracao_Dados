{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dparaujo/Mineracao_Dados/blob/main/Trabalho_Minera%C3%A7%C3%A3o_de_Dados_David_Araujo2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RWMQNlkDtC2-"
      },
      "source": [
        "Trabalho de Mineração de Dados (EDA)\n",
        "\n",
        "**Dataset:** BBC News\n",
        "\n",
        "\n",
        "*   https://www.kaggle.com/datasets/gpreda/bbc-news\n",
        "*   https://www.kaggle.com/code/gpreda/bbc-news-rss-feeds\n",
        "*   https://www.kaggle.com/datasets/pariza/bbc-news-summary/data\n",
        "*   https://storage.googleapis.com/dataset-uploader/bbc/bbc-text.csv\n",
        "\n",
        "Outros datasets:\n",
        "\n",
        "*   https://www.kaggle.com/datasets/shivamkushwaha/bbc-full-text-document-classification/code\n",
        "*   https://www.kaggle.com/c/learn-ai-bbc/overview\n",
        "*   https://www.kaggle.com/datasets/hgultekin/bbcnewsarchive\n",
        "*   https://www.kaggle.com/datasets/sahilkirpekar/bbcnews-dataset\n",
        "*   https://www.kaggle.com/code/warcoder/chromadb-semantic-search\n",
        "*   https://www.kaggle.com/code/anubhavgoyal10/getting-started-with-hugging-face\n",
        "*   https://www.kaggle.com/datasets/khushikyad001/fake-news-detection/data\n",
        "*   https://www.kaggle.com/datasets/mahdimashayekhi/fake-news-detection-dataset\n",
        "*   https://github.com/payamesfandiari/fake_news_finder\n",
        "*   https://www.kaggle.com/code/asif00/text-generation-with-tensorflow-nlp-rnn\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tf5rERlrtgZ0"
      },
      "source": [
        "### ***Alguns Testes:***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XpgU80jxturM"
      },
      "source": [
        "1. Preparação do Ambiente (Google Colab).\n",
        "Instalando e importando as bibliotecas necessárias:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IRqw7qqFnQRh"
      },
      "outputs": [],
      "source": [
        "# Instalação das bibliotecas\n",
        "!pip install pandas numpy seaborn matplotlib wordcloud nltk sentence-transformers faiss-cpu\n",
        "# !pip install openai # opcional se quiser usar API da OpenAI\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RZM-8rfS9h6a"
      },
      "outputs": [],
      "source": [
        "!pip install openai==0.28"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nCMwnPbjGWr8"
      },
      "outputs": [],
      "source": [
        "# from google.colab import userdata\n",
        "# userdata.get('HF_TOKEN')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hiecheq1tsDU"
      },
      "outputs": [],
      "source": [
        "# Importação das bibliotecas\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from wordcloud import WordCloud\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.util import ngrams\n",
        "from collections import Counter\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import faiss\n",
        "\n",
        "nltk.download('stopwords')\n",
        "stop_words = set(stopwords.words('english'))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dpbkVZejuGzN"
      },
      "source": [
        "2. Carregamento e Inspeção do Dataset BBC News.\n",
        "Dataset diretamente do Kaggle ou de outro link direto:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vIOFrUYduKWE"
      },
      "outputs": [],
      "source": [
        "# Exemplo com URL direta do CSV\n",
        "url = \"https://storage.googleapis.com/dataset-uploader/bbc/bbc-text.csv\"\n",
        "df = pd.read_csv(url)\n",
        "\n",
        "# Primeiras linhas\n",
        "df.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "background_save": true
        },
        "id": "5rtLBZDy6GS6"
      },
      "outputs": [],
      "source": [
        "# @title category\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "import seaborn as sns\n",
        "df.groupby('category').size().plot(kind='barh', color=sns.palettes.mpl_palette('Dark2'))\n",
        "plt.gca().spines[['top', 'right',]].set_visible(False)\n",
        "plt.xlabel('Quantidade')\n",
        "plt.ylabel('Categorias')\n",
        "plt.savefig('category.png', bbox_inches='tight', dpi=600)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hi57mnw5My-m"
      },
      "outputs": [],
      "source": [
        "# Plotando um histograma\n",
        "\n",
        "df[\"category\"].hist()\n",
        "\n",
        "plt.xlabel('Category')\n",
        "plt.ylabel('Amount')\n",
        "plt.savefig('category-histograma.png', bbox_inches='tight', dpi=600)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lHRsknGFN5Qd"
      },
      "outputs": [],
      "source": [
        "df\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "stNGv-btnFAz"
      },
      "outputs": [],
      "source": [
        "# @title category\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "import seaborn as sns\n",
        "df.groupby('category').size().plot(kind='barh', color=sns.palettes.mpl_palette('Dark2'))\n",
        "plt.gca().spines[['top', 'right',]].set_visible(False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wADyTaKp6LVr"
      },
      "outputs": [],
      "source": [
        "df.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DBD2bn5F-CKQ"
      },
      "outputs": [],
      "source": [
        "df.size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dUYp0frk-Esp"
      },
      "outputs": [],
      "source": [
        "df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1bGAGESB6O9e"
      },
      "outputs": [],
      "source": [
        "# df.mean()\n",
        "# df.max()\n",
        "# df.min()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nJleUFNzuPTc"
      },
      "source": [
        "3. Análise Exploratória de Dados (EDA)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I53h5oY7uiuc"
      },
      "source": [
        "a) Distribuição das Categorias"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rVrsDKHduSAc"
      },
      "outputs": [],
      "source": [
        "# Gráfico da Distribuição das Categorias\n",
        "\n",
        "plt.figure(figsize=(8,5))\n",
        "sns.countplot(y='category', data=df, order=df['category'].value_counts().index)\n",
        "plt.title('Distribuição das Categorias')\n",
        "plt.xlabel('Quantidade')\n",
        "plt.ylabel('Categorias')\n",
        "plt.savefig('distro_category.png', bbox_inches='tight', dpi=600)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cMZ9ReajuZrm"
      },
      "source": [
        "b) Histograma e Boxplot para comprimento dos textos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s0vTUMw9um31"
      },
      "outputs": [],
      "source": [
        "# Gráficos do Histograma e Boxplot\n",
        "df['text_length'] = df['text'].apply(lambda x: len(x.split()))\n",
        "\n",
        "# Histograma\n",
        "plt.figure(figsize=(10,4))\n",
        "sns.histplot(df['text_length'], bins=30, kde=True)\n",
        "plt.title('Distribuição do Comprimento dos Textos')\n",
        "plt.xlabel('Quantidade de palavras')\n",
        "plt.ylabel('Frequência')\n",
        "plt.savefig('hist_length.png', bbox_inches='tight', dpi=600)\n",
        "plt.show()\n",
        "\n",
        "# Boxplot\n",
        "plt.figure(figsize=(10,4))\n",
        "sns.boxplot(x='category', y='text_length', data=df)\n",
        "# plt.title('Boxplot de Comprimento dos Textos por Categoria')\n",
        "plt.xlabel('Categorias')\n",
        "plt.ylabel('Quantidade de palavras')\n",
        "plt.xticks(rotation=45)\n",
        "plt.savefig('boxplot_length.png', bbox_inches='tight', dpi=600)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TuUx0HNJu3Y6"
      },
      "source": [
        "c) Wordcloud (Nuvem de Palavras)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mKrZSzVvu4O0"
      },
      "outputs": [],
      "source": [
        "# Gráfico Nuvem de Palavras\n",
        "text = ' '.join(df['text']).lower()\n",
        "words = [word for word in text.split() if word not in stop_words]\n",
        "\n",
        "wordcloud = WordCloud(width=800, height=400).generate(' '.join(words))\n",
        "\n",
        "plt.figure(figsize=(10,5))\n",
        "plt.imshow(wordcloud, interpolation='bilinear')\n",
        "plt.axis('off')\n",
        "plt.title('Wordcloud das Palavras mais Frequentes')\n",
        "plt.savefig('wordcloud.png', bbox_inches='tight', dpi=600)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oNycfWyovCAs"
      },
      "source": [
        "d) Distribuição de N-grams (ex.: bigramas)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d88Hg_afvCiN"
      },
      "outputs": [],
      "source": [
        "# Gráfico da Distribuição de N=grams\n",
        "bigrams = list(ngrams(words, 2))\n",
        "bigram_counts = Counter(bigrams).most_common(10)\n",
        "\n",
        "bigram_df = pd.DataFrame(bigram_counts, columns=['bigram', 'count'])\n",
        "bigram_df['bigram'] = bigram_df['bigram'].apply(lambda x: ' '.join(x))\n",
        "\n",
        "sns.barplot(y='bigram', x='count', data=bigram_df)\n",
        "plt.title('Top 10 Bigramas Mais Frequentes')\n",
        "plt.xlabel('Frequência')\n",
        "plt.ylabel('Bigrama')\n",
        "plt.savefig('top_bigrams.png', bbox_inches='tight', dpi=600)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0KOKxZjSxPRc"
      },
      "source": [
        "4. Aplicando a técnica RAG (Retrieval-Augmented Generation)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E29WywIIxQ5M"
      },
      "source": [
        "a) Criação dos embeddings dos textos"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "\n",
        "# # import os\n",
        "# # os.environ[\"HF_HUB_OFFLINE\"] = \"HF_TOKEN\"\n",
        "# # model = SentenceTransformer(\"path/to/all-MiniLM-L6-v2\")\n",
        "\n",
        "# model = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\", token=False)\n",
        "\n",
        "# # Criando embeddings\n",
        "# embeddings = model.encode(df['text'].tolist())\n",
        "# print(embeddings)\n",
        "\n",
        "# # Criação do índice FAISS\n",
        "# dimension = embeddings.shape[1]\n",
        "# index = faiss.IndexFlatL2(dimension)\n",
        "# index.add(np.array(embeddings))"
      ],
      "metadata": {
        "id": "E3iV12binPj4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eNRIOwP-xR7s"
      },
      "outputs": [],
      "source": [
        "model = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\", token=False)\n",
        "\n",
        "# criando embeddings\n",
        "embeddings = model.encode(df['text'].tolist())\n",
        "print(embeddings)\n",
        "\n",
        "# criação do índice FAISS\n",
        "dimension = embeddings.shape[1]\n",
        "index = faiss.IndexFlatL2(dimension)\n",
        "index.add(np.array(embeddings))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FkjjUaptxV70"
      },
      "source": [
        "b) Realizar busca semântica com RAG"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LWDV-aRaxWYE"
      },
      "outputs": [],
      "source": [
        "def retrieve_documents(question, top_k=5):\n",
        "    query_embedding = model.encode([question])\n",
        "    distances, indices = index.search(query_embedding, top_k)\n",
        "    return df.iloc[indices[0]]\n",
        "\n",
        "# exemplo teste:\n",
        "query = \"What happened recently in UK politics?\"\n",
        "retrieved_docs = retrieve_documents(query)\n",
        "print(retrieved_docs[['category', 'text']])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z8gsaz3ZQ4PN"
      },
      "outputs": [],
      "source": [
        "# Exemple2:\n",
        "# query = \"What the inferency or abstract the last five articles about technology?\"\n",
        "query = \"Please provide a one-paragraph summary of your interpretation of the last five technology articles?\"\n",
        "retrieved_docs = retrieve_documents(query)\n",
        "print(retrieved_docs[['category', 'text']])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uRJnBvhxF6LE"
      },
      "source": [
        "**Sem RAG**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C253m6pNF5y4"
      },
      "outputs": [],
      "source": [
        "def retrieve_documents_semRAG(question, top_k=5):\n",
        "    query_embedding = model.encode([question])\n",
        "    distances, indices = index.search(query_embedding, top_k)\n",
        "    return df.iloc[indices[0]]\n",
        "\n",
        "# Exemplo:\n",
        "query = \"What happened recently in UK politics?\"\n",
        "retrieved_docs = retrieve_documents_semRAG(query)\n",
        "print(retrieved_docs[['category', 'text']])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iJ6ufFVrGb2A"
      },
      "source": [
        "**Sem utilizar técnica de geração textual (RAG). Apenas a recuperação semântica baseada em embeddings e similaridade:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XoD-3Hg1GbSZ"
      },
      "outputs": [],
      "source": [
        "\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import faiss\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Carrega o modelo de embeddings\n",
        "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "\n",
        "# Dataset de textos\n",
        "df = pd.read_csv(\"https://storage.googleapis.com/dataset-uploader/bbc/bbc-text.csv\")\n",
        "\n",
        "# Geração dos embeddings dos documentos\n",
        "corpus = df['text'].tolist()\n",
        "document_embeddings = model.encode(corpus, show_progress_bar=True)\n",
        "\n",
        "# Criação do índice FAISS\n",
        "index = faiss.IndexFlatL2(document_embeddings.shape[1])\n",
        "index.add(np.array(document_embeddings))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T1Anzhi-Gtji"
      },
      "source": [
        "**Função de busca semântica sem RAG**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8UJQNnjDGqm8"
      },
      "outputs": [],
      "source": [
        "def semantic_search(query, top_k=5):\n",
        "    # Codifica a consulta como embedding\n",
        "    query_embedding = model.encode([query])\n",
        "    # Busca os top_k documentos mais próximos no índice FAISS\n",
        "    distances, indices = index.search(query_embedding, top_k)\n",
        "    # Retorna os documentos mais semelhantes\n",
        "    return df.iloc[indices[0]]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bRBA-F7-G0Bf"
      },
      "source": [
        "**Executar a busca**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UqZhePa5G0y4"
      },
      "outputs": [],
      "source": [
        "query = \"What happened recently in UK politics?\"\n",
        "results = semantic_search(query, top_k=5)\n",
        "print(results[['category', 'text']])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OGCajViDxX1M"
      },
      "source": [
        "c) Gerar respostas com LLM (Opcional usando OpenAI GPT)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VTlZH3WGxZ5s"
      },
      "outputs": [],
      "source": [
        "import openai\n",
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "# openai.api_key = 'SUA_API_KEY'\n",
        "# openai.api_key = 'OPENAI_TOKEN'\n",
        "# openai.api_key = userdata.get('OPENAI_API_KEY')\n",
        "openai.api_key = userdata.get('OPENAI_TOKEN')\n",
        "\n",
        "os.environ[\"API_TOKEN\"] = userdata.get('OPENAI_TOKEN')\n",
        "\n",
        "\n",
        "def generate_answer(question, context_texts):\n",
        "    prompt = f\"\"\"\n",
        "    Context: {' '.join(context_texts)}\n",
        "\n",
        "    Question: {question}\n",
        "\n",
        "    Answer:\n",
        "    \"\"\"\n",
        "\n",
        "    response = openai.ChatCompletion.create(\n",
        "        model='gpt-3.5-turbo',\n",
        "        messages=[{'role': 'user', 'content': prompt}]\n",
        "    )\n",
        "\n",
        "    return response['choices'][0]['message']['content']\n",
        "\n",
        "# Exemplo prático\n",
        "contexts = retrieved_docs['text'].tolist()\n",
        "# contexts = retrieve_documents['text'].tolist()\n",
        "answer = generate_answer(query, contexts)\n",
        "print(answer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bAwrOyP9zl_M"
      },
      "source": [
        "## **Tarefa de Classificação com BBC News:**\n",
        "\n",
        "**Objetivo:** Classificar textos de notícias em categorias (ex.: política, negócios, esportes, tecnologia, entretenimento).\n",
        "\n",
        "**Estrutura do dataset:**\n",
        "\n",
        "*   Coluna text: Texto integral das notícias.\n",
        "*   Coluna category: Rótulo da classe de cada notícia.\n",
        "\n",
        "**Tipo de Classificação:** Multiclasse.\n",
        "\n",
        "**Exemplo de classes:**\n",
        "\n",
        "*   business\n",
        "*   politics\n",
        "*   sport\n",
        "*   tech\n",
        "*   entertainment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_lOVplFH0TlM"
      },
      "source": [
        "Exemplo usando Python (Scikit-learn):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gRnU0fVS0SjO"
      },
      "outputs": [],
      "source": [
        "# Importando bibliotecas\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Dividindo dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    df['text'], df['category'], test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "# Vetorização TF-IDF\n",
        "vectorizer = TfidfVectorizer(stop_words='english', max_features=5000)\n",
        "X_train_vec = vectorizer.fit_transform(X_train)\n",
        "X_test_vec = vectorizer.transform(X_test)\n",
        "\n",
        "# Classificação usando Naive Bayes\n",
        "clf = MultinomialNB()\n",
        "clf.fit(X_train_vec, y_train)\n",
        "\n",
        "# Avaliando o modelo\n",
        "y_pred = clf.predict(X_test_vec)\n",
        "print(classification_report(y_test, y_pred))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "biGD3bPc01y7"
      },
      "source": [
        "### **Como combinar RAG com Classificação?**\n",
        "\n",
        "Embora RAG seja tradicionalmente usado para geração textual baseada em recuperação, é possível usá-lo de maneira indireta para auxiliar na tarefa de classificação:\n",
        "\n",
        "**Usar embeddings de RAG para aprimorar a representação dos textos:**\n",
        "Os embeddings usados no RAG (ex.: Sentence-BERT) podem ser diretamente usados\n",
        "\n",
        "*   como entrada para classificadores mais avançados (ex.: Redes Neurais, SVM, ou Random Forest)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mq5tbPR_06_s"
      },
      "outputs": [],
      "source": [
        "# Embeddings\n",
        "embeddings = model.encode(df['text'].tolist())\n",
        "\n",
        "# Classificação usando embeddings com Random Forest\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    embeddings, df['category'], test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "clf_rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "clf_rf.fit(X_train, y_train)\n",
        "\n",
        "y_pred = clf_rf.predict(X_test)\n",
        "print(classification_report(y_test, y_pred))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F9SFGxKJ2SIM"
      },
      "source": [
        "## **Passo a Passo Completo em Python no Google Colab**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E--4P9Um2Ubk"
      },
      "source": [
        "**1. Instalação e importação das bibliotecas**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_tHkVEP_2XEc"
      },
      "outputs": [],
      "source": [
        "# Instalar bibliotecas necessárias\n",
        "!pip install pandas numpy matplotlib seaborn nltk wordcloud sentence-transformers scikit-learn faiss-cpu\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E6jofBAo2cwN"
      },
      "outputs": [],
      "source": [
        "# Importações\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from wordcloud import WordCloud\n",
        "from collections import Counter\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import faiss\n",
        "\n",
        "nltk.download('stopwords')\n",
        "stop_words = set(stopwords.words('english'))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WkN-1Pw02bEL"
      },
      "source": [
        "**2. Carregar o Dataset BBC News**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lWqAaWed2g-8"
      },
      "outputs": [],
      "source": [
        "# Carregar dataset diretamente do link CSV\n",
        "url = 'https://storage.googleapis.com/dataset-uploader/bbc/bbc-text.csv'\n",
        "df = pd.read_csv(url)\n",
        "\n",
        "# Exibir primeiras linhas\n",
        "df.head()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rvq384Bb2igc"
      },
      "source": [
        "**3. Análise Exploratória de Dados (EDA)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "edeNMhYO2tcc"
      },
      "source": [
        "3.1. Estrutura do Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XqndF1WX2kcM"
      },
      "outputs": [],
      "source": [
        "print(f\"Forma dos dados: {df.shape}\")\n",
        "print(\"Categorias disponíveis:\", df['category'].unique())\n",
        "df.info()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Ad5BMHf2mOs"
      },
      "source": [
        "3.2. Distribuição das Categorias (Gráfico de barras)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xO3F1zSy20-U"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(8,5))\n",
        "sns.countplot(y='category', data=df, order=df['category'].value_counts().index, palette='viridis')\n",
        "plt.title('Distribuição das Categorias')\n",
        "plt.xlabel('Quantidade de Artigos')\n",
        "plt.ylabel('Categorias')\n",
        "plt.savefig('distro_category2.png', bbox_inches='tight', dpi=600)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ef_uTZhO22X8"
      },
      "source": [
        "3.3. Distribuição do Comprimento dos Textos (Histograma e Boxplot)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3zg598XP24-k"
      },
      "outputs": [],
      "source": [
        "df['text_length'] = df['text'].apply(lambda x: len(x.split()))\n",
        "\n",
        "plt.figure(figsize=(10,5))\n",
        "sns.histplot(df['text_length'], bins=30, kde=True)\n",
        "plt.title('Distribuição do Comprimento dos Textos')\n",
        "plt.xlabel('Número de palavras')\n",
        "plt.ylabel('Frequência')\n",
        "plt.savefig('hist_length2.png', bbox_inches='tight', dpi=600)\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(10,6))\n",
        "sns.boxplot(x='category', y='text_length', data=df, palette='pastel')\n",
        "plt.title('Boxplot do Comprimento dos Textos por Categoria')\n",
        "plt.xticks(rotation=45)\n",
        "plt.savefig('boxplot_length2.png', bbox_inches='tight', dpi=600)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4qXRhae126UM"
      },
      "source": [
        "3.4. Nuvem de Palavras (Wordcloud)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p_sTEp8928SM"
      },
      "outputs": [],
      "source": [
        "text = ' '.join(df['text']).lower()\n",
        "filtered_words = [word for word in text.split() if word not in stop_words]\n",
        "\n",
        "wordcloud = WordCloud(width=800, height=400, background_color='white').generate(' '.join(filtered_words))\n",
        "\n",
        "plt.figure(figsize=(10,6))\n",
        "plt.imshow(wordcloud, interpolation='bilinear')\n",
        "plt.axis('off')\n",
        "plt.title('Wordcloud das Palavras mais Frequentes')\n",
        "plt.savefig('wordcloud2.png', bbox_inches='tight', dpi=600)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VqeMKBjU29uU"
      },
      "source": [
        "3.5. Bigramas Mais Frequentes (N-Grams)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IGqjh1Zo2_oE"
      },
      "outputs": [],
      "source": [
        "from nltk.util import ngrams\n",
        "\n",
        "bigrams = list(ngrams(filtered_words, 2))\n",
        "bigram_counts = Counter(bigrams).most_common(10)\n",
        "\n",
        "bigram_df = pd.DataFrame(bigram_counts, columns=['Bigram', 'Contagem'])\n",
        "bigram_df['Bigram'] = bigram_df['Bigram'].apply(lambda x: ' '.join(x))\n",
        "\n",
        "plt.figure(figsize=(10,5))\n",
        "sns.barplot(y='Bigram', x='Contagem', data=bigram_df, palette='coolwarm')\n",
        "plt.title('Top 10 Bigramas Mais Frequentes')\n",
        "plt.xlabel('Contagem')\n",
        "plt.ylabel('Bigrama')\n",
        "plt.savefig('top_bigrams2.png', bbox_inches='tight', dpi=600)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZxVPqyn0KqhW"
      },
      "outputs": [],
      "source": [
        "from nltk.util import ngrams\n",
        "\n",
        "bigrams = list(ngrams(filtered_words, 3))\n",
        "bigram_counts = Counter(bigrams).most_common(10)\n",
        "\n",
        "bigram_df = pd.DataFrame(bigram_counts, columns=['Bigram', 'Contagem'])\n",
        "bigram_df['Bigram'] = bigram_df['Bigram'].apply(lambda x: ' '.join(x))\n",
        "\n",
        "plt.figure(figsize=(10,5))\n",
        "sns.barplot(y='Bigram', x='Contagem', data=bigram_df, palette='coolwarm')\n",
        "plt.title('Top 10 Bigramas Mais Frequentes')\n",
        "plt.xlabel('Contagem')\n",
        "plt.ylabel('Bigrama')\n",
        "plt.savefig('top_trigrams2.png', bbox_inches='tight', dpi=600)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iXBK0pCl3BLs"
      },
      "source": [
        "**4. Aplicação da Técnica RAG (embeddings com Sentence-BERT)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x8w2PVWh3FWM"
      },
      "source": [
        "4.1. Criação dos Embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SZr5Iw3G3Fx1"
      },
      "outputs": [],
      "source": [
        "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "\n",
        "# Criando embeddings\n",
        "embeddings = model.encode(df['text'].tolist())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pASfUr6W3JiE"
      },
      "source": [
        "4.2. Criação do índice de busca semântica (FAISS)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c1T8gcme3J9c"
      },
      "outputs": [],
      "source": [
        "dimension = embeddings.shape[1]\n",
        "index = faiss.IndexFlatL2(dimension)\n",
        "index.add(np.array(embeddings))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gLbsdv6X3YwU"
      },
      "source": [
        "4.3. Exemplo de Recuperação (RAG simplificado)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "msLl-7iA3ZQ0"
      },
      "outputs": [],
      "source": [
        "def retrieve_docs(query, top_k=5):\n",
        "    query_embedding = model.encode([query])\n",
        "    distances, indices = index.search(query_embedding, top_k)\n",
        "    return df.iloc[indices[0]]\n",
        "\n",
        "# Exemplo\n",
        "# question = \"What recent technology developments were reported?\"\n",
        "question = \"What the inferency or abstract the last five articles about technology?\"\n",
        "results = retrieve_docs(question)\n",
        "\n",
        "print(results[['category', 'text']].head())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QmNtKRSm3c50"
      },
      "source": [
        "**5. Classificação Textual usando Embeddings**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8rDjSAsS3d8r"
      },
      "source": [
        "5.1. Divisão dos Dados (Treinamento e Teste)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "76KCQRX53fs0"
      },
      "outputs": [],
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    embeddings, df['category'], test_size=0.3, random_state=42\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ZUnTH723hQM"
      },
      "source": [
        "5.2. Treinamento com Random Forest (classificador robusto)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bt1FgS3r3jCE"
      },
      "outputs": [],
      "source": [
        "clf = RandomForestClassifier(n_estimators=200, random_state=42)\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Previsões\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "# Avaliação detalhada\n",
        "print(classification_report(y_test, y_pred))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "56Pq5FPX4KNF"
      },
      "source": [
        "5.3 Relatório"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZrAPWGU_4Qus"
      },
      "outputs": [],
      "source": [
        "# Importação adicional necessária\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
        "\n",
        "# Calculando as métricas de classificação\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision, recall, f1_score, _ = precision_recall_fscore_support(y_test, y_pred, average=None, labels=clf.classes_)\n",
        "\n",
        "# Criando dataframe para visualização\n",
        "metrics_df = pd.DataFrame({\n",
        "    'Categoria': clf.classes_,\n",
        "    'Precisão': precision,\n",
        "    'Recall': recall,\n",
        "    'F1-Score': f1_score\n",
        "})\n",
        "\n",
        "print(\"Acurácia geral do modelo: {:.2f}%\".format(accuracy * 100))\n",
        "# print(\"\\n\"+metrics_df)\n",
        "print(metrics_df)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2cuXkJVl4Rr0"
      },
      "source": [
        "1. Gráfico de Acurácia Geral"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oWwcnuTL4cdE"
      },
      "outputs": [],
      "source": [
        "# Gráfico de Acurácia Geral\n",
        "plt.figure(figsize=(6,4))\n",
        "sns.barplot(x=['Acurácia Geral'], y=[accuracy*100], palette='Greens')\n",
        "plt.ylim(0,100)\n",
        "plt.ylabel('Acurácia (%)')\n",
        "plt.title('Acurácia Geral do Modelo')\n",
        "for i in range(1):\n",
        "    plt.text(i, accuracy*100 + 1, f'{accuracy*100:.2f}%', ha='center')\n",
        "    plt.savefig('acuracia_geral2.png', bbox_inches='tight', dpi=600)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7yGtoJjW4d-8"
      },
      "source": [
        "2. Gráfico de Precisão por Categoria"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t0J5XPqh4gWE"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10,5))\n",
        "sns.barplot(x='Categoria', y='Precisão', data=metrics_df, palette='Blues_d')\n",
        "plt.ylim(0,1)\n",
        "plt.title('Precisão por Categoria')\n",
        "plt.ylabel('Precisão')\n",
        "plt.xlabel('Categoria')\n",
        "plt.xticks(rotation=45)\n",
        "for i, p in enumerate(precision):\n",
        "    plt.text(i, p + 0.01, f'{p:.2f}', ha='center')\n",
        "    plt.savefig('precisao_por_categoria2.png', bbox_inches='tight', dpi=600)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M7XnSM-Z4hwk"
      },
      "source": [
        "3. Gráfico de Recall por Categoria"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SiExelb14mEE"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10,5))\n",
        "sns.barplot(x='Categoria', y='Recall', data=metrics_df, palette='Oranges_d')\n",
        "plt.ylim(0,1)\n",
        "plt.title('Recall por Categoria')\n",
        "plt.ylabel('Recall')\n",
        "plt.xlabel('Categoria')\n",
        "plt.xticks(rotation=45)\n",
        "for i, r in enumerate(recall):\n",
        "    plt.text(i, r + 0.01, f'{r:.2f}', ha='center')\n",
        "    plt.savefig('recall_por_categoria2.png', bbox_inches='tight', dpi=600)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ORh1PjL54njE"
      },
      "source": [
        "4. Gráfico de F1-Score por Categoria"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2JLUnJQU4p5k"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10,5))\n",
        "sns.barplot(x='Categoria', y='F1-Score', data=metrics_df, palette='Purples_d')\n",
        "plt.ylim(0,1)\n",
        "plt.title('F1-Score por Categoria')\n",
        "plt.ylabel('F1-Score')\n",
        "plt.xlabel('Categoria')\n",
        "plt.xticks(rotation=45)\n",
        "for i, f1 in enumerate(f1_score):\n",
        "    plt.text(i, f1 + 0.01, f'{f1:.2f}', ha='center')\n",
        "    plt.savefig('f1_score_por_categoria2.png', bbox_inches='tight', dpi=600)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CX8uMT6K3mw9"
      },
      "source": [
        "**6. Avaliação gráfica dos Resultados**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p6fEMBSq3nuM"
      },
      "source": [
        "Matriz de Confusão"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mz9xzjf13pVE"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "cm = confusion_matrix(y_test, y_pred, labels=clf.classes_)\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=clf.classes_, yticklabels=clf.classes_)\n",
        "plt.title('Matriz de Confusão')\n",
        "plt.xlabel('Previsto')\n",
        "plt.ylabel('Real')\n",
        "plt.xticks(rotation=45)\n",
        "plt.yticks(rotation=0)\n",
        "plt.savefig('matriz_confusao2.png', bbox_inches='tight', dpi=600)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jlwnED7AKJ7z"
      },
      "source": [
        "Guia prático **completo e detalhado** (sem embeddings) para realizar uma **análise exploratória (EDA), visualizações gráficas** e **classificação textual** dos datasets **BBC News**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kp0VyGZbKsNz"
      },
      "source": [
        "**1. Configuração inicial no Google Colab**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aGv6Ci_-Kn40"
      },
      "outputs": [],
      "source": [
        "# Instalar bibliotecas necessárias\n",
        "!pip install pandas numpy matplotlib seaborn nltk wordcloud scikit-learn\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8AYAdFgiK9ub"
      },
      "outputs": [],
      "source": [
        "# Importações essenciais\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from wordcloud import WordCloud\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, precision_recall_fscore_support\n",
        "\n",
        "nltk.download('stopwords')\n",
        "stop_words = set(stopwords.words('english'))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "513u1vtILAUb"
      },
      "source": [
        "**2. Carregar e Explorar os Datasets**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vKzG9lngLB57"
      },
      "source": [
        "2.1. Dataset BBC News"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cRmWw-nNLBMs"
      },
      "outputs": [],
      "source": [
        "url_bbc = 'https://storage.googleapis.com/dataset-uploader/bbc/bbc-text.csv'\n",
        "df_bbc = pd.read_csv(url_bbc)\n",
        "print(df_bbc.head())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rg5DsmcdLImb"
      },
      "source": [
        "**3. EDA (Análise Exploratória) – Exemplo para BBC News**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bZVuKyl1LMiq"
      },
      "outputs": [],
      "source": [
        "# Distribuição das categorias\n",
        "sns.countplot(y='category', data=df_bbc, palette='Set2')\n",
        "plt.title('Distribuição Categorias BBC News')\n",
        "plt.xlabel('Quantidade')\n",
        "plt.ylabel('Categoria')\n",
        "plt.savefig('distro_category_bbc.png', bbox_inches='tight', dpi=600)\n",
        "plt.show()\n",
        "\n",
        "# Comprimento dos textos\n",
        "df_bbc['text_length'] = df_bbc['text'].apply(lambda x: len(x.split()))\n",
        "sns.histplot(df_bbc['text_length'], bins=30, kde=True)\n",
        "plt.title('Comprimento dos Textos BBC News')\n",
        "plt.xlabel('Número de Palavras')\n",
        "plt.ylabel('Frequência')\n",
        "plt.savefig('hist_length_bbc.png', bbox_inches='tight', dpi=600)\n",
        "plt.show()\n",
        "\n",
        "# Wordcloud\n",
        "text = ' '.join(df_bbc['text']).lower()\n",
        "filtered_words = [word for word in text.split() if word not in stop_words]\n",
        "wordcloud = WordCloud(width=800, height=400).generate(' '.join(filtered_words))\n",
        "plt.imshow(wordcloud, interpolation='bilinear')\n",
        "plt.axis('off')\n",
        "plt.title('Wordcloud BBC News')\n",
        "plt.savefig('wordcloud_bbc.png', bbox_inches='tight', dpi=600)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_-1qik96LJi7"
      },
      "source": [
        "**4. Classificação sem Embeddings (TF-IDF)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qzw-JzHgLcDk"
      },
      "outputs": [],
      "source": [
        "# Dividir os dados\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    df_bbc['text'], df_bbc['category'], test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "# Vetorização TF-IDF\n",
        "tfidf = TfidfVectorizer(stop_words='english', max_features=5000)\n",
        "X_train_vec = tfidf.fit_transform(X_train)\n",
        "X_test_vec = tfidf.transform(X_test)\n",
        "\n",
        "# Classificador Random Forest\n",
        "clf_rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "clf_rf.fit(X_train_vec, y_train)\n",
        "y_pred = clf_rf.predict(X_test_vec)\n",
        "\n",
        "# Avaliação\n",
        "print(classification_report(y_test, y_pred))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ffsg7YrqLdBj"
      },
      "source": [
        "**5. Avaliação gráfica das métricas (exemplo BBC News)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aUvFm_3sLiS8"
      },
      "source": [
        "**Matriz de Confusão**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZmSV5F32LgLr"
      },
      "outputs": [],
      "source": [
        "cm = confusion_matrix(y_test, y_pred, labels=clf_rf.classes_)\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=clf_rf.classes_, yticklabels=clf_rf.classes_)\n",
        "plt.title('Matriz de Confusão BBC News (sem embeddings)')\n",
        "plt.xlabel('Previsto')\n",
        "plt.ylabel('Real')\n",
        "# plt.xticks(rotation=45)\n",
        "# plt.yticks(rotation=0)\n",
        "plt.savefig('matriz_confusao_bbc.png', bbox_inches='tight', dpi=600)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9gDM3qDnLlvr"
      },
      "source": [
        "**Precisão, Recall, F1-Score**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rIrfaob3Lonj"
      },
      "outputs": [],
      "source": [
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision, recall, f1_score, _ = precision_recall_fscore_support(y_test, y_pred, labels=clf_rf.classes_)\n",
        "\n",
        "metrics_df = pd.DataFrame({\n",
        "    'Categoria': clf_rf.classes_,\n",
        "    'Precisão': precision,\n",
        "    'Recall': recall,\n",
        "    'F1-Score': f1_score\n",
        "})\n",
        "\n",
        "print(f\"Acurácia geral: {accuracy*100:.2f}%\")\n",
        "print(metrics_df)\n",
        "\n",
        "# Gráfico das métricas\n",
        "metrics_df.set_index('Categoria').plot.bar(rot=0, figsize=(10,6), colormap='Pastel1')\n",
        "plt.title('Precisão, Recall e F1-Score BBC News (sem embeddings)')\n",
        "plt.ylabel('Valor')\n",
        "plt.ylim(0,1)\n",
        "plt.grid(axis='y')\n",
        "plt.savefig('precisao_recall_f1_bbc.png', bbox_inches='tight', dpi=600)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PuDWvaKQ6KNR"
      },
      "outputs": [],
      "source": [
        "# Instalação\n",
        "!pip install pandas numpy seaborn matplotlib nltk wordcloud scikit-learn sentence-transformers tensorflow keras\n",
        "\n",
        "# Importações\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from wordcloud import WordCloud\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, precision_recall_fscore_support\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import tensorflow as tf\n",
        "\n",
        "nltk.download('stopwords')\n",
        "stop_words = set(stopwords.words('english'))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Majhn1A26TSC"
      },
      "outputs": [],
      "source": [
        "url_bbc = 'https://storage.googleapis.com/dataset-uploader/bbc/bbc-text.csv'\n",
        "df = pd.read_csv(url_bbc)\n",
        "df.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DDtyiuLW6Yyg"
      },
      "outputs": [],
      "source": [
        "# Distribuição categorias\n",
        "sns.countplot(y='category', data=df)\n",
        "plt.title('Distribuição das Categorias')\n",
        "plt.show()\n",
        "\n",
        "# Wordcloud\n",
        "text = ' '.join(df['text']).lower()\n",
        "wordcloud = WordCloud(width=800, height=400).generate(text)\n",
        "plt.imshow(wordcloud, interpolation='bilinear')\n",
        "plt.axis('off')\n",
        "plt.title('Wordcloud BBC News')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FR3mtmuH6eyL"
      },
      "source": [
        "## **4. Preparação dos dados**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DeaHLP-V6i-L"
      },
      "source": [
        "**Sem embeddings (TF-IDF)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gHconk6P6gOw"
      },
      "outputs": [],
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(df['text'], df['category'], test_size=0.3, random_state=42)\n",
        "\n",
        "vectorizer = TfidfVectorizer(stop_words='english', max_features=5000)\n",
        "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
        "X_test_tfidf = vectorizer.transform(X_test)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5YdySJVx6mWS"
      },
      "source": [
        "**Com embeddings (Sentence-BERT)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iDNPotT46o7b"
      },
      "outputs": [],
      "source": [
        "# model_emb = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "model_emb = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\", token=False)\n",
        "\n",
        "X_embeddings = model_emb.encode(df['text'])\n",
        "\n",
        "X_train_emb, X_test_emb, y_train_emb, y_test_emb = train_test_split(\n",
        "    X_embeddings, df['category'], test_size=0.3, random_state=42\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XeBpwp9m6vei"
      },
      "source": [
        "## **5. Classificação sem Embeddings (Naive Bayes e SVM)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Xi4aeKm6zO5"
      },
      "source": [
        "**Naive Bayes**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9z_5PSP_6wpx"
      },
      "outputs": [],
      "source": [
        "nb_clf = MultinomialNB()\n",
        "nb_clf.fit(X_train_tfidf, y_train)\n",
        "y_pred_nb = nb_clf.predict(X_test_tfidf)\n",
        "print(\"Naive Bayes sem embeddings:\\n\", classification_report(y_test, y_pred_nb))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SXaiuZlm61eD"
      },
      "source": [
        "**Support Vector Machines (SVM)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bzfxhQcs63V4"
      },
      "outputs": [],
      "source": [
        "svm_clf = SVC()\n",
        "svm_clf.fit(X_train_tfidf, y_train)\n",
        "y_pred_svm = svm_clf.predict(X_test_tfidf)\n",
        "print(\"SVM sem embeddings:\\n\", classification_report(y_test, y_pred_svm))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_KjnJDpg7kJ5"
      },
      "source": [
        "## **6. Classificação com Embeddings (Naive Bayes e SVM)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GAq1yYKu7rzb"
      },
      "source": [
        "**Naive Bayes com embeddings (GaussianNB)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E11-xzLB7lVy"
      },
      "outputs": [],
      "source": [
        "from sklearn.naive_bayes import GaussianNB\n",
        "\n",
        "gnb_clf = GaussianNB()\n",
        "gnb_clf.fit(X_train_emb, y_train_emb)\n",
        "y_pred_gnb = gnb_clf.predict(X_test_emb)\n",
        "print(\"Naive Bayes com embeddings:\\n\", classification_report(y_test_emb, y_pred_gnb))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t1iqV9xi761R"
      },
      "source": [
        "**SVM com embeddings**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m6S6EN5H76_x"
      },
      "outputs": [],
      "source": [
        "svm_emb_clf = SVC()\n",
        "svm_emb_clf.fit(X_train_emb, y_train_emb)\n",
        "y_pred_svm_emb = svm_emb_clf.predict(X_test_emb)\n",
        "print(\"SVM com embeddings:\\n\", classification_report(y_test_emb, y_pred_svm_emb))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OZDaxgpl8S1J"
      },
      "source": [
        "## **7. Avaliação gráfica (exemplo: Naive Bayes sem embeddings)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WAinx3Ns8UAq"
      },
      "outputs": [],
      "source": [
        "cm = confusion_matrix(y_test, y_pred_nb, labels=nb_clf.classes_)\n",
        "sns.heatmap(cm, annot=True, cmap='Blues', fmt='d', xticklabels=nb_clf.classes_, yticklabels=nb_clf.classes_)\n",
        "plt.title('Matriz de Confusão – Naive Bayes sem embeddings')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ijd6Q95h8irR"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import confusion_matrix, accuracy_score, precision_recall_fscore_support\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "def plot_confusion_matrix(y_true, y_pred, labels, title='Matriz de Confusão'):\n",
        "    cm = confusion_matrix(y_true, y_pred, labels=labels)\n",
        "    plt.figure(figsize=(8,6))\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "                xticklabels=labels, yticklabels=labels)\n",
        "    plt.title(title)\n",
        "    plt.xlabel('Previsto')\n",
        "    plt.ylabel('Real')\n",
        "    plt.show()\n",
        "\n",
        "def plot_metrics(y_true, y_pred, labels, model_name, title_suffix=''):\n",
        "    accuracy = accuracy_score(y_true, y_pred)\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(y_true, y_pred, labels=labels)\n",
        "\n",
        "    df_metrics = pd.DataFrame({\n",
        "        'Categoria': labels,\n",
        "        'Precisão': precision,\n",
        "        'Recall': recall,\n",
        "        'F1-Score': f1\n",
        "    })\n",
        "\n",
        "    print(f\"Acurácia Geral – {model_name} {title_suffix}: {accuracy*100:.2f}%\")\n",
        "    display(df_metrics)\n",
        "\n",
        "    df_metrics.set_index('Categoria').plot.bar(rot=0, figsize=(10,6))\n",
        "    plt.title(f'{model_name} – Métricas por Categoria {title_suffix}')\n",
        "    plt.ylabel('Valor')\n",
        "    plt.ylim(0, 1)\n",
        "    plt.grid(axis='y')\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CV1saKQN8nmR"
      },
      "source": [
        "## **2. Aplicar para Modelos SEM Embeddings (TF-IDF)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vuBzBvb18qh5"
      },
      "source": [
        "** Random Forest sem embeddings**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AZ_u_a4a8otW"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "rf_tfidf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf_tfidf.fit(X_train_tfidf, y_train)\n",
        "y_pred_rf = rf_tfidf.predict(X_test_tfidf)\n",
        "\n",
        "plot_confusion_matrix(y_test, y_pred_rf, labels=rf_tfidf.classes_, title='Random Forest – Sem Embeddings')\n",
        "plot_metrics(y_test, y_pred_rf, rf_tfidf.classes_, model_name='Random Forest', title_suffix='(Sem Embeddings)')\n",
        "plot_confusion_matrix(y_test, y_pred_rf, labels=rf_tfidf.classes_, )\n",
        "plot_metrics(y_test, y_pred_rf, rf_tfidf.classes_, model_name='Random Forest',)\n",
        "plt.savefig('matriz_rf_tfidf2.png', bbox_inches='tight', dpi=600)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P7jSWWhL87Tx"
      },
      "source": [
        "** Naive Bayes sem embeddings**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L3bpZ8v288KJ"
      },
      "outputs": [],
      "source": [
        "from sklearn.naive_bayes import MultinomialNB\n",
        "\n",
        "nb_tfidf = MultinomialNB()\n",
        "nb_tfidf.fit(X_train_tfidf, y_train)\n",
        "y_pred_nb = nb_tfidf.predict(X_test_tfidf)\n",
        "\n",
        "# plot_confusion_matrix(y_test, y_pred_nb, labels=nb_tfidf.classes_, title='Naive Bayes – Sem Embeddings')\n",
        "# plot_metrics(y_test, y_pred_nb, nb_tfidf.classes_, model_name='Naive Bayes', title_suffix='(Sem Embeddings)')\n",
        "plot_confusion_matrix(y_test, y_pred_nb, labels=nb_tfidf.classes_, )\n",
        "plot_metrics(y_test, y_pred_nb, nb_tfidf.classes_, model_name='Naive Bayes', )\n",
        "plt.savefig('matriz_rf_NB.png', bbox_inches='tight', dpi=300)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JWHAZXeI9BqJ"
      },
      "source": [
        "**SVM sem embeddings**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iv4J76a39Cfh"
      },
      "outputs": [],
      "source": [
        "from sklearn.svm import SVC\n",
        "\n",
        "svm_tfidf = SVC()\n",
        "svm_tfidf.fit(X_train_tfidf, y_train)\n",
        "y_pred_svm = svm_tfidf.predict(X_test_tfidf)\n",
        "\n",
        "# plot_confusion_matrix(y_test, y_pred_svm, labels=svm_tfidf.classes_, title='SVM – Sem Embeddings')\n",
        "# plot_metrics(y_test, y_pred_svm, svm_tfidf.classes_, model_name='SVM', title_suffix='(Sem Embeddings)')\n",
        "plot_confusion_matrix(y_test, y_pred_svm, labels=svm_tfidf.classes_, )\n",
        "plot_metrics(y_test, y_pred_svm, svm_tfidf.classes_, model_name='SVM', )\n",
        "plt.savefig('matriz_rf_SVM.png', bbox_inches='tight', dpi=300)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fPFMp3sI9L65"
      },
      "source": [
        "## **3. Aplicar para Modelos COM Embeddings**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KwhsrGbn9PEZ"
      },
      "source": [
        "**Random Forest com embeddings**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9tw114t59NCA"
      },
      "outputs": [],
      "source": [
        "rf_emb = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf_emb.fit(X_train_emb, y_train_emb)\n",
        "y_pred_rf_emb = rf_emb.predict(X_test_emb)\n",
        "\n",
        "# plot_confusion_matrix(y_test_emb, y_pred_rf_emb, labels=rf_emb.classes_, title='Random Forest – Com Embeddings')\n",
        "# plot_metrics(y_test_emb, y_pred_rf_emb, rf_emb.classes_, model_name='Random Forest', title_suffix='(Com Embeddings)')\n",
        "plot_confusion_matrix(y_test_emb, y_pred_rf_emb, labels=rf_emb.classes_, )\n",
        "plot_metrics(y_test_emb, y_pred_rf_emb, rf_emb.classes_, model_name='Random Forest', )\n",
        "plt.savefig('matriz_RF_com.png', bbox_inches='tight', dpi=300)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hTr-WDru9SZJ"
      },
      "source": [
        "**Naive Bayes com embeddings**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vOie9edZ9U3p"
      },
      "outputs": [],
      "source": [
        "from sklearn.naive_bayes import GaussianNB\n",
        "\n",
        "nb_emb = GaussianNB()\n",
        "nb_emb.fit(X_train_emb, y_train_emb)\n",
        "y_pred_nb_emb = nb_emb.predict(X_test_emb)\n",
        "\n",
        "# plot_confusion_matrix(y_test_emb, y_pred_nb_emb, labels=nb_emb.classes_, title='Naive Bayes – Com Embeddings')\n",
        "# plot_metrics(y_test_emb, y_pred_nb_emb, nb_emb.classes_, model_name='Naive Bayes', title_suffix='(Com Embeddings)')\n",
        "plot_confusion_matrix(y_test_emb, y_pred_nb_emb, labels=nb_emb.classes_, )\n",
        "plot_metrics(y_test_emb, y_pred_nb_emb, nb_emb.classes_, model_name='Naive Bayes', )\n",
        "plt.savefig('matriz_NB_com.png', bbox_inches='tight', dpi=300)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yOeffTkd9Zki"
      },
      "source": [
        "**SVM com embeddings**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "izhR4YxH9eK7"
      },
      "outputs": [],
      "source": [
        "svm_emb = SVC()\n",
        "svm_emb.fit(X_train_emb, y_train_emb)\n",
        "y_pred_svm_emb = svm_emb.predict(X_test_emb)\n",
        "\n",
        "# plot_confusion_matrix(y_test_emb, y_pred_svm_emb, labels=svm_emb.classes_, title='SVM – Com Embeddings')\n",
        "# plot_metrics(y_test_emb, y_pred_svm_emb, svm_emb.classes_, model_name='SVM', title_suffix='(Com Embeddings)')\n",
        "plot_confusion_matrix(y_test_emb, y_pred_svm_emb, labels=svm_emb.classes_,)\n",
        "plot_metrics(y_test_emb, y_pred_svm_emb, svm_emb.classes_, model_name='SVM',)\n",
        "plt.savefig('matriz_SVM_com.png', bbox_inches='tight', dpi=300)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "authorship_tag": "ABX9TyO3AWI0TbwBnzhS14pjP9ek",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}